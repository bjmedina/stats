{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro to Autograd\n",
    "\n",
    "This notebook will get you started with autograd, a powerful tool for taking derivatives of arbitrary functions. The two most popular frameworks for implementing autograd are Pytorch and Tensorflow. We will be using Pytorch because it is more user-friendly.\n",
    "\n",
    "This framework is particularly important for calculating derivatives of deep learning functions, but it can also be useful for classical models like logistic regression. This notebook covers a basic example of gradient descent in a 1D state space (you will do the same in 2D for your homework), and we will cover more complex uses of autograd (for example, to train a model) later on in the course."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-2.3000,  4.0000])\n",
      "tensor([-2.6000, 10.0000]) tensor([-12.1670,  64.0000])\n",
      "tensor([1.1000, 1.0000]) tensor([-0.6765, -1.3333])\n"
     ]
    }
   ],
   "source": [
    "# import the pytorch module (called torch)\n",
    "import torch\n",
    "\n",
    "# the basic unit of pytorch is a tensor (works almost the same as numpy array, but with extra features)\n",
    "x_tensor = torch.tensor([-2.3, 4])\n",
    "print(x_tensor)\n",
    "\n",
    "# you can use scalars and built-in python math ops as you normally would\n",
    "print(2 * (x_tensor + 1), x_tensor ** 3)\n",
    "# for vector addition or multiplication, etc. must be tensor of same size\n",
    "y_tensor = torch.tensor([3.4, -3])\n",
    "print(x_tensor + y_tensor, x_tensor / y_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradients can be calculated for operations that use x:  True\n"
     ]
    }
   ],
   "source": [
    "# a function that we want to minimize (must be written to accept and return a torch tensor)\n",
    "def f(x_var):\n",
    "    y = (x_var + torch.Tensor([1.0])) ** 2\n",
    "    return y\n",
    "\n",
    "# initialize x (we will start from x = 3.63)\n",
    "x_0 = torch.tensor([3.63])\n",
    "\n",
    "# gradient descent parameters\n",
    "step_sz = 0.35\n",
    "num_steps = 20\n",
    "\n",
    "# make an autograd variable for x (after performing operations with x, the gradient can be passed back to x)\n",
    "x = torch.autograd.Variable(x_0, requires_grad=True)\n",
    "# if x.requires_grad is True, this means it is \"activated\" for autograd computation\n",
    "print(\"Gradients can be calculated for operations that use x: \", x.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for loop for iterative gradient descent\n",
    "for step in range(num_steps):\n",
    "    # get the gradient (torch.autograd.grad returns 1-element tuple so need to grab element 0 of the output)\n",
    "    x_grad = torch.autograd.grad(f(x), x)[0]\n",
    "    \n",
    "    # gradient descent update. (a -= b) translates to (a = a - b)\n",
    "    x.data -= step_sz * x_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradients can be calculated for operations that use x:  False\n",
      "Final minimizer from gradient descent: x =  tensor([-1.])\n"
     ]
    }
   ],
   "source": [
    "# remove x from the autograd process (not necessary for this example, but usually turn of autograd when done)\n",
    "x = x.detach()\n",
    "# if x.requires_grad is False, this means it is a normal tensor\n",
    "print(\"Gradients can be calculated for operations that use x: \", x.requires_grad)\n",
    "# print result. the minimizer of f is x = -1, which is exactly what we get (up to some small numerical error)\n",
    "print(\"Final minimizer from gradient descent: x = \", x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are other ways to use autograd that we will look at later. The basic idea behind autograd is that a \"computational graph\" is built everytime that an operation is performed with a variable that has the `requires_grad=True` attribute. In this example, the graph starts at `x` and ends at `f(x)`. Once the end of the graph is reached, gradients can be passed from the end of the graph to beginning of the graph (or an intermediate point) using a technique called \"backpropagation\", whixh is a fancy term for a computataional implementaion of the Chain Rule from calculus. \n",
    "\n",
    "We will discuss the details more later, but for now feel free to play around with autograd or look into a more detailed explanation [here](https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html). The Pytorch tutorial shows you how to do backpropagation using the `.backward()` function instead of `torch.autograd.grad`, but the idea is exactly the same."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
